{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:13.687027900Z",
     "start_time": "2024-12-09T19:38:13.674363100Z"
    }
   },
   "id": "c572d4f947995d08",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:32.050368200Z",
     "start_time": "2024-12-09T19:38:32.034368900Z"
    }
   },
   "id": "59a814d3644c24f6",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Source directories\n",
    "main_dir = '../data/Corona'  # Replace with your directory path\n",
    "covid_dir = os.path.join(main_dir, 'covid')\n",
    "normal_dir = os.path.join(main_dir, 'normal')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:32.761536100Z",
     "start_time": "2024-12-09T19:38:32.753541200Z"
    }
   },
   "id": "b9ba42b032636bdc",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to create train/val/test splits\n",
    "def create_data_splits(base_dir):\n",
    "    # Create directories for splits\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    classes = ['covid', 'normal']\n",
    "    \n",
    "    for split in splits:\n",
    "        for cls in classes:\n",
    "            os.makedirs(os.path.join(base_dir, split, cls), exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:33.536807200Z",
     "start_time": "2024-12-09T19:38:33.514809400Z"
    }
   },
   "id": "3b1a53958c5a9da2",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    # Create temporary directory for splits\n",
    "    temp_dir = 'temp_dataset'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    create_data_splits(temp_dir)\n",
    "    \n",
    "    # Function to split and copy files\n",
    "    def process_class_files(class_dir, class_name):\n",
    "        files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # First split: train + remaining\n",
    "        train_files, temp_files = train_test_split(files, test_size=0.3, random_state=42)\n",
    "        # Second split: validation + test\n",
    "        val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Copy files to respective directories\n",
    "        for file, split in zip([train_files, val_files, test_files], ['train', 'validation', 'test']):\n",
    "            for f in file:\n",
    "                src = os.path.join(class_dir, f)\n",
    "                dst = os.path.join(temp_dir, split, class_name, f)\n",
    "                shutil.copy2(src, dst)\n",
    "                \n",
    "        return len(train_files), len(val_files), len(test_files)\n",
    "    # Process both classes\n",
    "    covid_counts = process_class_files(covid_dir, 'covid')\n",
    "    normal_counts = process_class_files(normal_dir, 'normal')\n",
    "    \n",
    "    print(\"Dataset split complete:\")\n",
    "    print(f\"COVID images - Train: {covid_counts[0]}, Validation: {covid_counts[1]}, Test: {covid_counts[2]}\")\n",
    "    print(f\"Normal images - Train: {normal_counts[0]}, Validation: {normal_counts[1]}, Test: {normal_counts[2]}\")\n",
    "    \n",
    "    return temp_dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:34.293614600Z",
     "start_time": "2024-12-09T19:38:34.275232900Z"
    }
   },
   "id": "ab6d62208af0ce95",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "COVID images - Train: 48, Validation: 10, Test: 11\n",
      "Normal images - Train: 17, Validation: 4, Test: 4\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "dataset_dir = prepare_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:35.166805600Z",
     "start_time": "2024-12-09T19:38:35.025116500Z"
    }
   },
   "id": "e5dfd6be3275983",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:36.038475300Z",
     "start_time": "2024-12-09T19:38:36.018449900Z"
    }
   },
   "id": "f9eb4d637c25e7d2",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Only rescaling for validation and test\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:36.811489Z",
     "start_time": "2024-12-09T19:38:36.781769Z"
    }
   },
   "id": "e2442c33e91ecaba",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'train'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:37.589676700Z",
     "start_time": "2024-12-09T19:38:37.457334400Z"
    }
   },
   "id": "83011b13a31bc3b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'validation'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:38.336490600Z",
     "start_time": "2024-12-09T19:38:38.208070100Z"
    }
   },
   "id": "7de34eb0413707d0",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'test'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:39.195724200Z",
     "start_time": "2024-12-09T19:38:39.069779500Z"
    }
   },
   "id": "466b7f89f2f80e01",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Base model - Using EfficientNetB0 as feature extractor\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:39.762084200Z",
     "start_time": "2024-12-09T19:38:39.755634900Z"
    }
   },
   "id": "6941a2d9a57d3a15",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = build_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:43.153992100Z",
     "start_time": "2024-12-09T19:38:40.689379500Z"
    }
   },
   "id": "c0658ceea43dccbd",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:44.020068600Z",
     "start_time": "2024-12-09T19:38:43.993761200Z"
    }
   },
   "id": "77c7cd3d13d843f2",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:44.952504800Z",
     "start_time": "2024-12-09T19:38:44.927708300Z"
    }
   },
   "id": "a8f51c81576b6d1",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:45.794260900Z",
     "start_time": "2024-12-09T19:38:45.762817100Z"
    }
   },
   "id": "88da2a5961557580",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:46.522109100Z",
     "start_time": "2024-12-09T19:38:46.498745800Z"
    }
   },
   "id": "8608d303f3fa394e",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8022 - accuracy: 0.6154 - auc_1: 0.5950 - precision_1: 0.3667 - recall_1: 0.6471\n",
      "Epoch 1: val_loss improved from inf to 0.70916, saving model to best_model.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18876\\4242241589.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_generator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mearly_stopping\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce_lr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m )\n",
      "\u001B[1;32m~\\Documents\\Uni\\DeepLearninProject\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\json\\__init__.py\u001B[0m in \u001B[0;36mdumps\u001B[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[0;32m    236\u001B[0m         \u001B[0mcheck_circular\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcheck_circular\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_nan\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mallow_nan\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mindent\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    237\u001B[0m         \u001B[0mseparators\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mseparators\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdefault\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msort_keys\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msort_keys\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 238\u001B[1;33m         **kw).encode(obj)\n\u001B[0m\u001B[0;32m    239\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\json\\encoder.py\u001B[0m in \u001B[0;36mencode\u001B[1;34m(self, o)\u001B[0m\n\u001B[0;32m    197\u001B[0m         \u001B[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m         \u001B[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 199\u001B[1;33m         \u001B[0mchunks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miterencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mo\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_one_shot\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    200\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchunks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    201\u001B[0m             \u001B[0mchunks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchunks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\json\\encoder.py\u001B[0m in \u001B[0;36miterencode\u001B[1;34m(self, o, _one_shot)\u001B[0m\n\u001B[0;32m    255\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkey_separator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem_separator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort_keys\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m                 self.skipkeys, _one_shot)\n\u001B[1;32m--> 257\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_iterencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mo\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001B[1;31mTypeError\u001B[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:38:58.995630500Z",
     "start_time": "2024-12-09T19:38:47.320793700Z"
    }
   },
   "id": "746d823c46ea7d47",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Convert EagerTensor to numpy arrays if needed\n",
    "    def convert_to_numpy(history_dict):\n",
    "        numpy_history = {}\n",
    "        for key, value in history_dict.items():\n",
    "            numpy_history[key] = np.array(value)\n",
    "        return numpy_history\n",
    "    \n",
    "    history_dict = convert_to_numpy(history.history)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history_dict['accuracy'])\n",
    "    ax1.plot(history_dict['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['Train', 'Validation'])\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history_dict['loss'])\n",
    "    ax2.plot(history_dict['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:02.833480300Z",
     "start_time": "2024-12-09T19:39:02.811315600Z"
    }
   },
   "id": "9efdfbadb9f3ac57",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "def evaluate_model(model, test_generator):\n",
    "    # Get predictions\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    # Reset the generator\n",
    "    test_generator.reset()\n",
    "    \n",
    "    # Predict in batches\n",
    "    for i in range(len(test_generator)):\n",
    "        x, y = test_generator[i]\n",
    "        pred = model.predict(x)\n",
    "        predictions.extend(pred)\n",
    "        labels.extend(y)\n",
    "        \n",
    "        if len(labels) >= len(test_generator.labels):\n",
    "            break\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels[:len(test_generator.labels)])\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred = (predictions > 0.5).astype(int)\n",
    "    y_true = labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(test_generator)\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Normal', 'COVID']))\n",
    "    \n",
    "    return y_true, y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:03.646836900Z",
     "start_time": "2024-12-09T19:39:03.609653Z"
    }
   },
   "id": "7c6c377e0c9d2620",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model_and_weights(model, model_name='covid19_detection_model'):\n",
    "    # Save model architecture as JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"{model_name}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Save weights\n",
    "    model.save_weights(f\"{model_name}_weights.h5\")\n",
    "    \n",
    "    print(f\"Model saved as {model_name}.json and {model_name}_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:04.338301500Z",
     "start_time": "2024-12-09T19:39:04.317261300Z"
    }
   },
   "id": "c9127633ed854de1",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "def fine_tune_model():\n",
    "    # Unfreeze some layers of the base model\n",
    "    base_model = model.layers[0]\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all layers except the last 30\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:05.049498100Z",
     "start_time": "2024-12-09T19:39:05.037692600Z"
    }
   },
   "id": "561b8c22db8c14b7",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: name 'history' is not defined\n"
     ]
    }
   ],
   "source": [
    "# After training, use these functions:\n",
    "try:\n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_true, y_pred = evaluate_model(model, test_generator)\n",
    "    \n",
    "    # Save model\n",
    "    save_model_and_weights(model)\n",
    "    \n",
    "    # Fine-tune model\n",
    "    fine_tuned_model = fine_tune_model()\n",
    "    history_fine_tune = fine_tuned_model.fit(\n",
    "        train_generator,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    # Plot fine-tuning history\n",
    "    plot_training_history(history_fine_tune)\n",
    "    \n",
    "    # Evaluate fine-tuned model\n",
    "    print(\"\\nFine-tuned Model Evaluation:\")\n",
    "    y_true_ft, y_pred_ft = evaluate_model(fine_tuned_model, test_generator)\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    save_model_and_weights(fine_tuned_model, 'covid19_detection_model_finetuned')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "finally:\n",
    "    # Clean up temporary directory\n",
    "    try:\n",
    "        shutil.rmtree(dataset_dir)\n",
    "    except:\n",
    "        print(\"Error while deleting temporary directory\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:05.879848Z",
     "start_time": "2024-12-09T19:39:05.820434Z"
    }
   },
   "id": "fb8299c96a2314ed",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp_dataset\\\\train\\\\normal\\\\NORMAL2-IM-1142-0001-0001.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18876\\395241456.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_generator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mearly_stopping\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce_lr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m )\n",
      "\u001B[1;32m~\\Documents\\Uni\\DeepLearninProject\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Uni\\DeepLearninProject\\.venv\\lib\\site-packages\\keras\\utils\\image_utils.py\u001B[0m in \u001B[0;36mload_img\u001B[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001B[0m\n\u001B[0;32m    420\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpathlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m             \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresolve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 422\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    423\u001B[0m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpil_image\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBytesIO\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'temp_dataset\\\\train\\\\normal\\\\NORMAL2-IM-1142-0001-0001.jpeg'"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "fine_tuned_model = fine_tune_model()\n",
    "history_fine_tune = fine_tuned_model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-09T19:39:07.337814400Z",
     "start_time": "2024-12-09T19:39:07.280647100Z"
    }
   },
   "id": "155df671f822e2ea",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f07cac26af42dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
